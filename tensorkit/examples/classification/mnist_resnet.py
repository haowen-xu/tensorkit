import mltk
import tensorkit as tk
from tensorkit import tensor as T
from tensorkit.examples import utils


class Config(mltk.Config):
    max_epoch: int = 10
    batch_size: int = 32
    test_batch_size: int = 64
    lr: float = 0.01
    lr_anneal_ratio: float = 0.5
    lr_anneal_epochs: int = 2


def main(exp: mltk.Experiment[Config]):
    # prepare the data
    train_stream, val_stream, test_stream = utils.get_mnist_streams(
        batch_size=exp.config.batch_size,
        test_batch_size=exp.config.test_batch_size,
        val_batch_size=exp.config.test_batch_size,
        val_portion=0.2,
        x_range=(-1., 1.),
    )

    # build the network
    net: T.Module = tk.layers.SequentialBuilder(train_stream.data_shapes[0]). \
        set_args('res_block2d',
                 kernel_size=3,
                 activation=tk.layers.LeakyReLU,
                 normalizer=tk.layers.BatchNorm2d,
                 dropout=0.5,
                 data_init=tk.init.StdDataInit()). \
        res_block2d(16). \
        res_block2d(32, stride=2). \
        res_block2d(32). \
        res_block2d(64, stride=2). \
        res_block2d(64). \
        global_avg_pool2d(). \
        linear(10). \
        log_softmax(). \
        build()

    # the train, test and validate functions
    def train_step(x, y):
        logits = net(x)
        loss = T.nn.cross_entropy_with_logits(logits, y, reduction='mean')
        acc = utils.calculate_acc(logits, y)
        return {'loss': loss, 'acc': acc}

    def evaluate(x, y):
        with T.no_grad():
            logits = net(x)
            acc = utils.calculate_acc(logits, y)
        return {'acc': acc}

    # build the optimizer and the train loop
    loop = mltk.TrainLoop(max_epoch=exp.config.max_epoch)
    optimizer = tk.optim.Adam(tk.layers.get_parameters(net))
    lr_scheduler = tk.optim.lr_scheduler.AnnealingLR(
        loop=loop,
        optimizer=optimizer,
        initial_lr=exp.config.lr,
        ratio=exp.config.lr_anneal_ratio,
        epochs=exp.config.lr_anneal_epochs
    )

    # add a callback to do early-stopping on the network parameters
    # according to the validation metric.
    loop.add_callback(
        mltk.callbacks.EarlyStopping(
            checkpoint=tk.train.Checkpoint(net=net),
            root_dir=exp.abspath('./checkpoint/early-stopping'),
            # note for `loop.validation()`, the prefix "val_" will be
            # automatically prepended to any metrics generated by the
            # `evaluate` function.
            metric_name='val_acc',
            smaller_is_better=False,
        )
    )

    # run validation after every epoch
    loop.run_after_every(
        lambda: loop.validation().run(evaluate, val_stream),
        epochs=1,
    )

    # run test after every epoch
    loop.run_after_every(
        lambda: loop.test().run(evaluate, test_stream),
        epochs=1,
    )

    # train the model
    utils.fit_model(loop=loop, optimizer=optimizer, fn=train_step,
                    stream=train_stream)

    # do the final test with the best network parameters (according to validation)
    results = mltk.TestLoop().run(evaluate, test_stream)


if __name__ == '__main__':
    with mltk.Experiment(Config) as exp:
        main(exp)
