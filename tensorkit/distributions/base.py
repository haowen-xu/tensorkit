from typing import *

from mltk.utils import DocInherit

from .. import tensor as T
from ..settings_ import settings
from .utils import *

__all__ = ['Distribution']


class Distribution(metaclass=DocInherit):
    """Base distribution class."""

    # Notes on implementing a sub-class of Distribution
    #
    # 1. Every distribution class must accept `event_ndims` and
    #    `validate_tensors` in its constructor.
    # 2. Every re-parameterizable distribution class must also accept
    #    `reparameterized` in its constructor.
    # 3. Sub-classes should generally follow the orders of the constructor
    #    arguments in `Distribution`.
    # 4. It is recommended to place `epsilon` constructor argument (if exist)
    #    before `validate_tensors`, but after all other arguments.

    dtype: str
    """Dtype of the samples."""

    continuous: bool
    """
    Whether or not the distribution is continuous (i.e., values of the samples
    are real numbers)?
    """

    reparameterized: bool
    """
    Whether or not the distribution is re-parameterized (i.e., gradients can
    be back-propagated to the parameters of this distribution via the samples)?
    """

    min_event_ndims: int
    """
    The minimum allowed number of dimensions of a single event, for all
    distributions of this distribution family. 
    """

    value_shape: Optional[List[int]]
    """
    Value shape of samples from this distribution.  This is the shape of the
    samples generated by :meth:`sample()`, when `n_samples` is :obj:`None`,
    where ``value_shape == batch_shape + event_shape`` should always hold.
    May not be known in some distribution classes. 
    """

    value_ndims: int
    """The number of dimensions of samples."""

    event_shape: Optional[List[int]]
    """
    Value shape of a single event from this distribution.
    May not be known in some distribution classes.
    """

    event_ndims: int
    """Number of dimensions of a single event from this distribution."""

    batch_shape: List[int]
    """
    Batch shape of this distribution.
    ``value_shape == batch_shape + event_shape``.
    """

    device: str
    """Device, where the parameters of this distribution is placed."""

    validate_tensors: bool
    """
    Whether or not to perform time-consuming validation on argument tensors
    and intermediate computation results?
    Defaults to ``settings.validate_tensors``.
    """

    def __init__(self,
                 dtype: str,
                 value_shape: Optional[List[int]] = None,
                 batch_shape: Optional[List[int]] = None,
                 continuous: Optional[bool] = None,
                 reparameterized: Optional[bool] = None,
                 event_ndims: Optional[int] = None,
                 min_event_ndims: Optional[int] = None,
                 device: Optional[str] = None,
                 validate_tensors: Optional[bool] = None):
        # either `value_shape` or `batch_shape` should be specified, but not both.
        if value_shape is None and batch_shape is None:
            raise ValueError('Either `value_shape` or `batch_shape` should be '
                             'specified.')

        # override class defaults
        if continuous is not None:
            continuous = bool(continuous)
            if hasattr(self, 'continuous') and self.continuous != continuous:
                raise ValueError('`continuous` has already been defined by '
                                 'class attribute, thus cannot be overrided.')
            self.continuous = continuous
        if min_event_ndims is not None:
            min_event_ndims = int(min_event_ndims)
            if hasattr(self, 'min_event_ndims') and \
                    self.min_event_ndims != min_event_ndims:
                raise ValueError('`min_event_ndims` has already been defined by'
                                 ' class attribute, thus cannot be overrided.')
            self.min_event_ndims = min_event_ndims
        if reparameterized is not None:
            # merge the `reparameterized` argument with the class level default
            self.reparameterized = get_overrided_parameterized(
                getattr(self, 'reparameterized', True),
                reparameterized,
                self.__class__
            )

        # set instance properties
        event_ndims = (
            self.min_event_ndims if event_ndims is None
            else int(event_ndims)
        )
        if event_ndims < self.min_event_ndims:
            raise ValueError(
                f'`event_ndims >= min_event_ndims` does not hold: '
                f'`event_ndims` == {event_ndims}, while '
                f'`min_event_ndims` == {self.min_event_ndims}'
            )

        if value_shape is not None:
            if event_ndims > len(value_shape):
                raise ValueError(
                    f'`event_ndims <= len(value_shape)` does not hold: '
                    f'`event_ndims` == {event_ndims}, while '
                    f'`len(value_shape)` == {len(value_shape)}'
                )
            value_shape = list(map(int, value_shape))

            the_batch_shape = value_shape[:len(value_shape) - event_ndims]
            if batch_shape is not None:
                if the_batch_shape != batch_shape:
                    raise ValueError(
                        f'The arguments `value_shape`, `batch_shape` and '
                        f'`event_ndims` are not coherent: `value_shape` is '
                        f'{value_shape}, `batch_shape` is {batch_shape}, '
                        f'while `event_ndims` is {event_ndims}.'
                    )
            batch_shape = the_batch_shape

            event_shape = value_shape[len(value_shape) - event_ndims:]
            value_ndims = len(value_shape)
        else:
            batch_shape = list(map(int, batch_shape))
            event_shape = None
            value_ndims = len(batch_shape) + event_ndims

        self.dtype = dtype
        self.value_shape = value_shape
        self.value_ndims = value_ndims
        self.batch_shape = batch_shape
        self.event_shape = event_shape
        self.event_ndims = event_ndims
        self.device = device or T.current_device()
        self.validate_tensors = (
            settings.validate_tensors if validate_tensors is None
            else bool(validate_tensors)
        )

    @property
    def base_distribution(self) -> 'Distribution':
        """
        Get the base distribution, from which this distribution is derived.


        Returns:
            The base distribution, if this distribution object is derived based
            on another distribution.  Otherwise returns this distribution itself.
        """
        return self

    @property
    def batch_ndims(self) -> int:
        """Get the number of batch dimensions."""
        return len(self.batch_shape)

    def _sample(self,
                n_samples: Optional[int],
                group_ndims: int,
                reduce_ndims: int,
                reparameterized: bool) -> 'StochasticTensor':
        raise NotImplementedError()

    def sample(self,
               n_samples: Optional[int] = None,
               group_ndims: int = 0,
               reparameterized: Optional[bool] = None) -> 'StochasticTensor':
        """
        Take samples from this distribution.

        This method will return a :class:`StochasticTensor` object, with
        both this distribution object and the generated samples sealed inside.
        For example::

            import tensorkit as tk
            from tensorkit import tensor as T

            normal = tk.Normal(mean=T.zeros([2, 3]), std=T.ones([2, 3]))
            samples = normal.sample(n_samples=4, group_ndims=1)
            samples.tensor  # get the sample values
            samples.log_prob()  # get the sample log-prob,
                # equivalent to ``normal.log_prob(t.tensor, group_ndims=1)``.

        Args:
            n_samples: The number of samples to take.  If specified, the shape
                of the samples will be ``[n_samples] + value_shape``.
                If not specified, the shape will be ``value_shape``.
            group_ndims: The default ``group_ndims`` to compute the log-prob
                of the samples.
            reparameterized: If specified, will overwrite ``is_parameterized``
                of this distribution.

        Returns:
            The samples, sealed in a :class:`StochasticTensor` object.
        """
        reduce_ndims = get_prob_reduce_ndims(
            self.value_ndims + int(n_samples is not None),
            self.min_event_ndims,
            self.event_ndims,
            group_ndims,
        )
        reparameterized = get_overrided_parameterized(
            self.reparameterized, reparameterized, self.__class__)
        return self._sample(
            n_samples, group_ndims, reduce_ndims, reparameterized)

    def _log_prob(self,
                  given: T.Tensor,
                  group_ndims: int,
                  reduce_ndims: int) -> T.Tensor:
        raise NotImplementedError()

    def log_prob(self,
                 given: 'TensorOrData',
                 group_ndims: int = 0) -> T.Tensor:
        """
        Compute the log-prob or log-density of the given tensor.

        Args:
            given: The given samples.
            group_ndims: The number of dimensions at the end of an event
                to be considered as a group of events.  The log-prob
                of each group of events will be summed up.

        Returns:
            The computed log-prob or log-density.
        """
        if not isinstance(given, T.Tensor):
            given = T.as_tensor(given, device=self.device)
        reduce_ndims = get_prob_reduce_ndims(
            # here `given` might have lower rank than `len(value_shape)`,
            # in which case `given` should be broadcasted to match `value_shape`.
            max(T.rank(given), self.value_ndims),
            self.min_event_ndims,
            self.event_ndims,
            group_ndims,
        )
        return self._log_prob(given, group_ndims, reduce_ndims)

    def prob(self,
             given: 'TensorOrData',
             group_ndims: int = 0) -> T.Tensor:
        """
        Compute the probability or density of the given tensor.

        Args:
            given: The given samples.
            group_ndims: The number of dimensions at the end of an event
                to be considered as a group of events.  The log-prob
                of each group of events will be summed up.

        Returns:
            The computed probability or density.
        """
        log_prob = self.log_prob(given, group_ndims)
        prob = T.exp(log_prob)
        if hasattr(log_prob, 'transform_origin'):
            # copy the additional attributes from `log_prob` to `prob`.
            prob.transform_origin = log_prob.transform_origin
        return prob

    def _assert_finite(self, tensor: T.Tensor, message: str) -> T.Tensor:
        if self.validate_tensors:
            tensor = T.assert_finite(tensor, message)
        return tensor

    def copy(self, **overrided_params):
        """
        Get a copy of this distribution instance, with overrided parameters.

        Args:
            \\**overrided_params: The overrided parameters.

        Returns:
            The copied distribution.
        """
        raise NotImplementedError()


# back reference to the StochasticTensor
from ..stochastic import StochasticTensor
from ..typing_ import TensorOrData
