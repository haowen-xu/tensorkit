from typing import *

from mltk.utils import DocInherit

from .. import tensor as T
from ..settings_ import settings
from .utils import *

__all__ = ['Distribution']


class Distribution(metaclass=DocInherit):
    """Base distribution class."""

    # Notes on implementing a sub-class of Distribution
    #
    # 1. Every distribution class must accept `event_ndims` and
    #    `validate_tensors` in its constructor.
    # 2. Every re-parameterizable distribution class must also accept
    #    `reparameterized` in its constructor.
    # 3. Sub-classes should generally follow the orders of the constructor
    #    arguments in `Distribution`.
    # 4. It is recommended to place `epsilon` constructor argument (if exist)
    #    before `validate_tensors`, but after all other arguments.

    dtype: str
    """Dtype of the samples."""

    continuous: bool
    """
    Whether or not the distribution is continuous (i.e., values of the samples
    are real numbers)?
    """

    reparameterized: bool
    """
    Whether or not the distribution is re-parameterized (i.e., gradients can
    be back-propagated to the parameters of this distribution via the samples)?
    """

    min_event_ndims: int
    """
    The minimum allowed number of dimensions of a single event, for all
    distributions of this distribution family. 
    """

    value_shape: List[int]
    """
    Value shape of samples from this distribution.  This is the shape of the
    samples generated by :meth:`sample()`, when `n_samples` is :obj:`None`,
    where ``value_shape == batch_shape + event_shape`` should always hold.  
    """

    event_shape: List[int]
    """Value shape of a single event from this distribution."""

    batch_shape: List[int]
    """
    Batch shape of this distribution.
    ``value_shape == batch_shape + event_shape``.
    """

    validate_tensors: bool
    """
    Whether or not to perform time-consuming validation on argument tensors
    and intermediate computation results?
    Defaults to ``settings.validate_tensors``.
    """

    def __init__(self,
                 dtype: str,
                 value_shape: List[int],
                 continuous: Optional[bool] = None,
                 reparameterized: Optional[bool] = None,
                 event_ndims: Optional[int] = None,
                 min_event_ndims: Optional[int] = None,
                 validate_tensors: Optional[bool] = None):
        # override class defaults
        if continuous is not None:
            continuous = bool(continuous)
            if hasattr(self, 'continuous') and self.continuous != continuous:
                raise ValueError('`continuous` has already been defined by '
                                 'class attribute, thus cannot be overrided.')
            self.continuous = continuous
        if min_event_ndims is not None:
            min_event_ndims = int(min_event_ndims)
            if hasattr(self, 'min_event_ndims') and \
                    self.min_event_ndims != min_event_ndims:
                raise ValueError('`min_event_ndims` has already been defined by'
                                 ' class attribute, thus cannot be overrided.')
            self.min_event_ndims = min_event_ndims
        if reparameterized is not None:
            # merge the `reparameterized` argument with the class level default
            self.reparameterized = get_overrided_parameterized(
                getattr(self, 'reparameterized', True),
                reparameterized,
                self.__class__
            )

        # set instance properties
        event_ndims = (
            self.min_event_ndims if event_ndims is None
            else int(event_ndims)
        )
        if event_ndims < self.min_event_ndims:
            raise ValueError(
                f'`event_ndims >= min_event_ndims` does not hold: '
                f'`event_ndims` == {event_ndims}, while '
                f'`min_event_ndims` == {self.min_event_ndims}'
            )
        if event_ndims > len(value_shape):
            raise ValueError(
                f'`event_ndims <= len(value_shape)` does not hold: '
                f'`event_ndims` == {event_ndims}, while '
                f'`len(value_shape)` == {len(value_shape)}'
            )

        value_shape = list(value_shape)
        self.dtype = dtype
        self.value_shape = value_shape
        self.batch_shape = value_shape[:len(value_shape) - event_ndims]
        self.event_shape = value_shape[len(value_shape) - event_ndims:]
        self.validate_tensors = (
            settings.validate_tensors if validate_tensors is None
            else bool(validate_tensors)
        )

    @property
    def base_distribution(self) -> 'Distribution':
        """
        Get the base distribution, from which this distribution is derived.


        Returns:
            The base distribution, if this distribution object is derived based
            on another distribution.  Otherwise returns this distribution itself.
        """
        return self

    @property
    def event_ndims(self) -> int:
        """
        The number of dimensions of a single event from this distribution,
        which should equal to ``len(value_shape)``.
        """
        return len(self.event_shape)

    def _sample(self,
                n_samples: Optional[int],
                group_ndims: int,
                reduce_ndims: int,
                reparameterized: bool) -> 'StochasticTensor':
        raise NotImplementedError()

    def sample(self,
               n_samples: Optional[int] = None,
               group_ndims: int = 0,
               reparameterized: Optional[bool] = None) -> 'StochasticTensor':
        """
        Take samples from this distribution.

        This method will return a :class:`StochasticTensor` object, with
        both this distribution object and the generated samples sealed inside.
        For example::

            import tensorkit as tk
            from tensorkit import tensor as T

            normal = tk.Normal(mean=T.zeros([2, 3]), std=T.ones([2, 3]))
            samples = normal.sample(n_samples=4, group_ndims=1)
            samples.tensor  # get the sample values
            samples.log_prob()  # get the sample log-prob,
                # equivalent to ``normal.log_prob(t.tensor, group_ndims=1)``.

        Args:
            n_samples: The number of samples to take.  If specified, the shape
                of the samples will be ``[n_samples] + value_shape``.
                If not specified, the shape will be ``value_shape``.
            group_ndims: The default ``group_ndims`` to compute the log-prob
                of the samples.
            reparameterized: If specified, will overwrite ``is_parameterized``
                of this distribution.

        Returns:
            The samples, sealed in a :class:`StochasticTensor` object.
        """
        reduce_ndims = get_prob_reduce_ndims(
            len(self.value_shape) + int(n_samples is not None),
            self.min_event_ndims,
            self.event_ndims,
            group_ndims,
        )
        reparameterized = get_overrided_parameterized(
            self.reparameterized, reparameterized, self.__class__)
        return self._sample(
            n_samples, group_ndims, reduce_ndims, reparameterized)

    def _log_prob(self,
                  given: T.Tensor,
                  group_ndims: int,
                  reduce_ndims: int) -> T.Tensor:
        raise NotImplementedError()

    def log_prob(self,
                 given: Union[T.Tensor, 'StochasticTensor'],
                 group_ndims: int = 0) -> T.Tensor:
        """
        Compute the log-prob or log-density of the given tensor.

        Args:
            given: The given samples.
            group_ndims: The number of dimensions at the end of an event
                to be considered as a group of events.  The log-prob
                of each group of events will be summed up.

        Returns:
            The computed log-prob or log-density.
        """
        if isinstance(given, StochasticTensor):
            given = given.tensor
        reduce_ndims = get_prob_reduce_ndims(
            # here `given` might have lower rank than `len(value_shape)`,
            # in which case `given` should be broadcasted to match `value_shape`.
            max(T.rank(given), len(self.value_shape)),
            self.min_event_ndims,
            self.event_ndims,
            group_ndims,
        )
        return self._log_prob(given, group_ndims, reduce_ndims)

    def prob(self,
             given: Union[T.Tensor, 'StochasticTensor'],
             group_ndims: int = 0) -> T.Tensor:
        """
        Compute the probability or density of the given tensor.

        Args:
            given: The given samples.
            group_ndims: The number of dimensions at the end of an event
                to be considered as a group of events.  The log-prob
                of each group of events will be summed up.

        Returns:
            The computed probability or density.
        """
        return T.exp(self.log_prob(given, group_ndims))

    def _assert_finite(self, tensor: T.Tensor, message: str) -> T.Tensor:
        if self.validate_tensors:
            tensor = T.assert_finite(tensor, message)
        return tensor

    def copy(self, **overrided_params):
        """
        Get a copy of this distribution instance, with overrided parameters.

        Args:
            \\**overrided_params: The overrided parameters.

        Returns:
            The copied distribution.
        """
        raise NotImplementedError()


# back reference to the StochasticTensor
from ..stochastic import StochasticTensor
